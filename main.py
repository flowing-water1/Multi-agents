import os
import streamlit as st

from langchain_community.callbacks.manager import get_openai_callback
from graph import create_llm, create_agent, agent_node, StateGraph, END, HumanMessage, python_repl, AgentState, router
import functools

from langchain_core.messages import (
    BaseMessage,
    HumanMessage,
    ToolMessage, AIMessage,
)
from langgraph.graph import END, StateGraph
from langgraph.prebuilt import ToolNode
from display import (
    extract_code_from_events,
    modify_code_for_streamlit,
    display_researcher_data,
    display_call_tool_data,
    display_chart_generator_data
)
from streamlit_check import (

    validate_and_set_keys,
    set_api_keys,
    delete_environment_variable,
    set_environment_variable
)
import sys
import io

# æ™ºè°±çš„
# b27f36511b74905f6adfd4f1e7cf1f72.r90Sh9TOsQ3JcQaf
# https://open.bigmodel.cn/api/paas/v4/

# using Tavily to research the question:'who is the richest person in this world now.', when you find it, finsish.ATTENTION:DO NOT USE 'chart_generator'
#Fetch the UK's GDP over the past 3 years, then draw a line graph of it. Once you code it up, finish.


# sk-9oYJRePIyAbz7wNj955dBbC98f0c44F8B91bF7779d38B131
# https://gtapi.xiaoerchaoren.com:8932
# https://gtapi.xiaoerchaoren.com:8932/v1
# https://gtapi.xiaoerchaoren.com:8932/v1/chat/completions

# https://tavily.com/
# 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
# https://smith.langchain.com/settings
# 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


# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”#

with st.sidebar:
    tab1, tab2 = st.tabs(["OPENAI", "æ™ºè°±"])
    with tab1:
        openai_api_key = st.text_input("OpenAI API Key:", type="password")
        openai_api_base = st.text_input("OpenAI API Base:")
        # openai_port = st.text_input("VPNçš„ç«¯å£ï¼š")
        st.markdown("[è·å–OpenAI API key](https://platform.openai.com/account/api-keys)")
        st.markdown("[OpenAI APIæ–‡æ¡£](https://platform.openai.com/docs/api-reference/introduction)")
        st.markdown("è¦ç”¨ç›´è¿åŸç‰ˆçš„APIè¯ï¼Œè¦å¼€VPNï¼Œç«¯å£è®¾ç½®ä¸º7890ã€‚")
        st.markdown("ç”¨ä¸­è½¬çš„ä¸ç”¨å¼€VPNï¼Œå·²æµ‹è¯•è¿‡ä¸­è½¬çš„è·Ÿç›´è¿çš„æ•ˆæœä¸€æ ·ã€‚")

    with tab2:
        zhipu_api_key = st.text_input("æ™ºè°±AIçš„API Key:", type="password")
        zhipu_api_base = st.text_input("æ™ºè°±AIçš„API Base:")
        st.markdown("[è·å–æ™ºè°±AIçš„API key](https://www.zhipuai.cn/)")
        st.markdown("å›½äº§çš„LLMæ¨¡å‹åŸºæœ¬ä¸Šæ— æ³•å®Œæˆä»»åŠ¡ï¼Œä½†æ˜¯å¯èƒ½å¯ä»¥é€šè¿‡ä¿®æ”¹promptå®Œæˆä»»åŠ¡")

st.title("Mutil-Agents Collaboration\n(For Research and Chart purpose)")
st.divider()
with st.expander("**:red[ä½¿ç”¨å‰å¿…çœ‹ä¿¡æ¯]**"):
    st.write('''**è¯·æ³¨æ„**ï¼ŒMutil-Agentsæ˜¯delicateçš„ï¼Œå› ä¸ºLLMæ¨¡å‹å†…éƒ¨æ˜¯:blue-background[é»‘ç®±].''')

    st.write(
        ''':blue-background[æ‰€ä»¥å¯èƒ½ä¼šæœ‰æ„æƒ³ä¸åˆ°çš„bugã€‚å°±ç®—æ˜¯åŒæ ·çš„APIï¼Œpromptå’Œæ¨¡å‹ï¼Œå¯èƒ½ç»“æœä¹Ÿä¼šæœ‰å·®è·ï¼Œè¿™æ˜¯ä¸å¯é¿å…çš„ã€‚]''')

    st.write('''***:red[æ¨¡å‹æ¨ègpt-4/gpt-4o/gpt-4-1106-preview/claude-3]***.''')

    st.write('''å®æµ‹è¿™äº›æ¨¡å‹å¯ä»¥å®Œæˆä»»åŠ¡ï¼ˆä»ç„¶ä¼šå—promptå½±å“å¯èƒ½æŠ¥é”™ï¼‰''')

    st.write('''**:red[tavilyæ˜¯è´Ÿè´£æœç´¢çš„ï¼Œlangchainæ˜¯ç›‘æ§æµç¨‹å’Œæä¾›æ”¯æŒçš„ï¼Œä¸€å®šè¦å¡«]**''')

    st.write(''':red[ä»¥ä¸‹æ˜¯è¿è¡ŒæˆåŠŸçš„ä¾‹å­ï¼Œå¯ä»¥å‚è€ƒ]''')
    st.image("1.png",use_column_width=True)

    st.image("2.png",use_column_width=True)

    st.image("3.png",use_column_width=True)

    st.image("4.png",use_column_width=True)

    st.image("5.png",use_column_width=True)

    st.image("6.png",use_column_width=True)

    st.image("7.png",use_column_width=True)

    st.image("8.png",use_column_width=True)


st.divider()

# åˆå§‹åŒ– session_state
if "tavily_api_key_set" not in st.session_state:
    st.session_state.tavily_api_key_set = bool(os.environ.get("TAVILY_API_KEY"))
    print("tavily_api_key_set:", st.session_state.tavily_api_key_set)

if "langchain_api_key_set" not in st.session_state:
    st.session_state.langchain_api_key_set = bool(os.environ.get("LANGCHAIN_API_KEY"))
    print("langchain_api_key_set:", st.session_state.langchain_api_key_set)

if "rerun" not in st.session_state:
    st.session_state.rerun = False


def set_environment_variable(key, value):
    os.environ[key] = value


# æ£€æµ‹å¹¶æ˜¾ç¤ºç¯å¢ƒå˜é‡çŠ¶æ€
column_check_tavily, column_check_langchain = st.columns([1, 1])

import os

# åˆå§‹åŒ– session_state

if "langchain_api_key_set" not in st.session_state:
    st.session_state.langchain_api_key_set = bool(os.environ.get("LANGCHAIN_API_KEY"))

if "tavily_api_key_set" not in st.session_state:
    st.session_state.tavily_api_key_set = bool(os.environ.get("TAVILY_API_KEY"))

if "show_dialog" not in st.session_state:
    st.session_state.show_dialog = False

if "dialog_active" not in st.session_state:
    st.session_state.dialog_active = False

if "langchain_toast_shown" not in st.session_state:
    st.session_state.langchain_toast_shown = False

if "tavily_toast_shown" not in st.session_state:
    st.session_state.tavily_toast_shown = False

if "langchain_api_key" not in st.session_state:
    st.session_state.langchain_api_key = ""

if "tavily_api_key" not in st.session_state:
    st.session_state.tavily_api_key = ""

if "tavily_tool" not in st.session_state:
    st.session_state.tavily_tool = None

if "show_rerun_button" not in st.session_state:
    st.session_state.show_rerun_button = False

# æ£€æµ‹å¹¶æ˜¾ç¤ºç¯å¢ƒå˜é‡çŠ¶æ€
if not st.session_state.langchain_api_key_set and not os.environ.get(
        "LANGCHAIN_API_KEY"):
    langchain_info = st.info("æœªè®¾ç½® LANGCHAIN_API_KEY")
elif os.environ.get(
        "LANGCHAIN_API_KEY") and not st.session_state.dialog_active and not st.session_state.langchain_toast_shown:
    st.toast("LANGCHAIN_API_KEY å·²åœ¨ç¯å¢ƒå˜é‡ä¸­é…ç½®æˆåŠŸ", icon='ğŸŒŸ')
    st.toast("LANGSMITH é…ç½®æˆåŠŸ", icon='ğŸŒŸ')
    st.session_state.langchain_toast_shown = True

if not st.session_state.tavily_api_key_set and not os.environ.get(
        "TAVILY_API_KEY"):
    tavily_info = st.info("æœªè®¾ç½® TAVILY_API_KEY")
elif os.environ.get(
        "TAVILY_API_KEY") and not st.session_state.dialog_active and not st.session_state.tavily_toast_shown:
    st.toast("TAVILY_API_KEY å·²åœ¨ç¯å¢ƒå˜é‡ä¸­æˆåŠŸ", icon='ğŸŒŸ')
    st.toast(":red[æ³¨æ„ï¼š]è¯·ç¡®ä¿ä½ çš„ TAVILY_API_KEY æœ‰æ•ˆï¼ŒTAVILY_TOOLå·¥å…·å¯ä»¥é…ç½®ï¼Œä½†ä¼¼ä¹åªä¼šåœ¨è¿è¡Œä¸­æŠ¥é”™ï¼Œ",
             icon='ğŸš«')
    st.session_state.tavily_toast_shown = True

# ä¾§è¾¹æ æŒ‰é’®è§¦å‘å¯¹è¯æ¡†
with st.sidebar:
    if st.button("è®¾ç½® LANGCHAIN_API_KEY å’Œ TAVILY_API_KEY"):
        st.session_state.show_dialog = True
        set_api_keys()

    if st.button("åˆ é™¤ç¯å¢ƒå˜é‡ä¸­çš„ LANGCHAIN_API_KEY"):
        delete_environment_variable("LANGCHAIN_API_KEY")
    if st.button("åˆ é™¤ç¯å¢ƒå˜é‡ä¸­çš„ TAVILY_API_KEY"):
        delete_environment_variable("TAVILY_API_KEY")


# æ•è·è¾“å‡ºçš„å‡½æ•°
# æ•è·è¾“å‡ºçš„å‡½æ•°
class OutputCatcher(io.StringIO):
    def __init__(self):
        super().__init__()
        self.stdout = sys.stdout
        self.stderr = sys.stderr
        self.initial_check_done = False

    def __enter__(self):
        sys.stdout = self
        sys.stderr = self
        return self

    def __exit__(self, *args):
        sys.stdout = self.stdout
        sys.stderr = self.stderr
        self.seek(0)
        output = self.read()
        if "Failed to batch ingest runs: LangSmithError" in output:
            st.toast("å†æ¬¡æé†’ï¼Œä½ çš„Langchain_Api_Keyæ˜¯é”™è¯¯çš„ï¼Œä½ å¯ä»¥å¿½ç•¥è¿™ä¸ªæé†’ï¼Œä½†æ˜¯å¦‚æœä½ è¦ç›‘æ§æµç¨‹ï¼ŒåŠ¡å¿…ä½¿ç”¨æ­£ç¡®çš„KEY",
                     icon="âš ï¸")
        self.close()

    def check_output(self):
        self.seek(0)
        output = self.read()
        if "Failed to batch ingest runs: LangSmithError" in output:
            st.toast("æ³¨æ„ï¼Œä½ çš„Langchain_Api_Keyæ˜¯é”™è¯¯çš„ï¼ŒLangSmithæ— æ³•é…ç½®ï¼Œä½†æ˜¯ä»ç„¶å¯ä»¥è¿è¡Œ", icon="âš ï¸")
        self.truncate(0)
        self.seek(0)


if not (openai_api_key or zhipu_api_key):
    st.info("è¯·è¾“å…¥OpenAI API Keyæˆ–è€…æ™ºè°±AIçš„API key")

if openai_api_key and zhipu_api_key:
    st.info("æœ‰ä¸¤ä¸ªapiï¼Œè¯·é€‰æ‹©ä¸€ä¸ªä½¿ç”¨å³å¯")

if (openai_api_key or zhipu_api_key) and (
        not os.environ.get("TAVILY_API_KEY") and not os.environ.get("LANGCHAIN_API_KEY")):
    st.info("ç¯å¢ƒå˜é‡ç¼ºå¤±:red[LANGCHAIN_API_KEY]å’Œ:red[TAVILY_API_KEY]")

if (
        openai_api_key and os.environ.get("LANGCHAIN_API_KEY") and os.environ.get("TAVILY_API_KEY") and not zhipu_api_key) or (
        zhipu_api_key and os.environ.get("LANGCHAIN_API_KEY") and os.environ.get("TAVILY_API_KEY") and not openai_api_key):
    column1, cloumn2 = st.columns([1, 1])
    with column1:
        research_agent_prompt = st.text_area('ä½ å¯ä»¥åœ¨è¿™ä¿®æ”¹:red[research_agent]çš„prompt(ç”¨äºæœç´¢æ•°æ®)\n\né»˜è®¤æ˜¯ï¼š',
                                             "You should provide accurate data for use, "
        "and source code shouldn't be the final answer",
                                             height=150

                                             )
        st.write("å¦‚æœä½ ä¸æ¸…æ¥šï¼Œå°½é‡ä¸è¦ä¿®æ”¹ï¼Œä¸Šè¿°çš„promptå·²éªŒè¯åœ¨:red[gpt-4/gpt-4-1106-preview]å¯è¿è¡Œ")

    with cloumn2:
        chart_agent_prompt = st.text_area('ä½ å¯ä»¥åœ¨è¿™ä¿®æ”¹:red[chart_agent]çš„promptï¼ˆç”¨äºç»˜å›¾ï¼‰\n\né»˜è®¤æ˜¯ï¼š',
                                          "Create the python code to display the chart."

                                          ,
                                          height=150)
        st.write("å¦‚æœä½ ä¸æ¸…æ¥šï¼Œå°½é‡ä¸è¦ä¿®æ”¹ï¼Œä¸Šè¿°çš„promptå·²éªŒè¯åœ¨:red[gpt-4/gpt-4-1106-preview]å¯è¿è¡Œ")

    st.divider()
    model_name = st.text_input("è¾“å…¥ä½ è¦ä½¿ç”¨çš„æ¨¡å‹:",placeholder= "gpt-4-1106-preview")
    human_prompt = st.text_area("è¯·è¾“å…¥ä½ æƒ³è¦å®Œæˆçš„æœç´¢å’Œç»˜åˆ¶å›¾è¡¨çš„ä»»åŠ¡",
                                placeholder=
                                "Fetch the UK's GDP over the "
                                "past 5 years, then draw a line "
                                "graph of it. Once you code it "
                                "up, finish.")
    start_button = st.button("å¼€å§‹")

    if start_button:
        if openai_api_key:
            api_key = openai_api_key
            api_base = openai_api_base

        else:
            api_key = zhipu_api_key
            api_base = zhipu_api_base

        # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” åˆ›å»º llm å®ä¾‹â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
        with st.spinner("åˆ›å»ºLLMå®ä¾‹å’ŒAgentså’ŒNodesä¸­ï¼Œè¯·ç¨ç­‰..."):
            all_total_tokens = 0

            llm = create_llm(model_name, api_key, api_base)

            # é‡æ–°åˆ›å»º agents å’Œ nodes
            research_agent = create_agent(
                llm,
                [st.session_state.tavily_tool],
                system_message=research_agent_prompt,
            )
            research_node = functools.partial(agent_node, agent=research_agent, name="Researcher")

            chart_agent = create_agent(
                llm,
                [python_repl],
                system_message=chart_agent_prompt,
            )
            chart_node = functools.partial(agent_node, agent=chart_agent, name="chart_generator")

            tools = [st.session_state.tavily_tool, python_repl]
            tool_node = ToolNode(tools)
            st.toast('åˆ›å»ºAgentså’ŒNodesæˆåŠŸå•¦~', icon='ğŸŒŸ')

        # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”åˆ›å»º workflowâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
        with st.spinner("åˆ›å»ºWorkflowå’ŒGraphä¸­ï¼Œè¯·ç¨ç­‰..."):

            workflow = StateGraph(AgentState)

            workflow.add_node("Researcher", research_node)
            workflow.add_node("chart_generator", chart_node)
            workflow.add_node("call_tool", tool_node)

            workflow.add_conditional_edges(
                "Researcher",
                router,
                {"continue": "chart_generator", "call_tool": "call_tool", "__end__": END},
            )
            workflow.add_conditional_edges(
                "chart_generator",
                router,
                {"continue": "Researcher", "call_tool": "call_tool", "__end__": END},
            )

            workflow.add_conditional_edges(
                "call_tool",
                # Each agent node updates the 'sender' field
                # the tool calling node does not, meaning
                # this edge will route back to the original agent
                # who invoked the tool
                lambda x: x["sender"],
                {
                    "Researcher": "Researcher",
                    "chart_generator": "chart_generator",
                },
            )
            workflow.set_entry_point("Researcher")
            st.toast('åˆ›å»ºWorkflowæˆåŠŸå•¦~', icon='ğŸŒŸ')
            graph = workflow.compile()
            total_tokens = 0  # Initialize token counter
            st.toast('åˆ›å»ºGraphæˆåŠŸå•¦~', icon='ğŸŒŸ')

        # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”è¿è¡ŒGraphâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
        try:
            with OutputCatcher() as output_catcher:
                with st.spinner("AIæ­£åœ¨æ€è€ƒä¸­ï¼Œè¯·ç¨ç­‰..."):

                    events = graph.stream(
                        {
                            "messages": [
                                HumanMessage(content=human_prompt)
                            ],
                        },
                        # Maximum number of steps to take in the graph
                        {"recursion_limit": 150},
                    )

                    with get_openai_callback() as cb:
                        event_list = []
                        code_detected = False
                        st.info("è¿è¡Œæ—¥å¿—ï¼š")
                        for s in events:
                            event_list.append(s)

                            if not output_catcher.initial_check_done:
                                output_catcher.check_output()
                                output_catcher.initial_check_done = True

                            # st.write(s)
                            # st.write("â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”")
                            display_researcher_data(event_list)  # åªæ˜¾ç¤ºResearcheréƒ¨åˆ†
                            display_call_tool_data(event_list)  # åªæ˜¾ç¤ºcall_tooléƒ¨åˆ†
                            display_chart_generator_data(event_list)  # åªæ˜¾ç¤ºchart_generatoréƒ¨åˆ†
                            code = extract_code_from_events(event_list)
                            if code:
                                st.toast('æå–ä»£ç æˆåŠŸå•¦~', icon='ğŸŒŸ')
                                modified_code = modify_code_for_streamlit(code)
                                exec(modified_code)
                                code_detected = True  # æ£€æµ‹åˆ°ä»£ç 

                            all_total_tokens += cb.total_tokens

                        if not code_detected:
                            st.error("æœªæ‰¾åˆ°åŒ…å«ä»£ç çš„æ—¥å¿—æ¡ç›®,ä½ å¯èƒ½å¯ä»¥ä»æ—¥å¿—ä¸­æ‰¾åˆ°ç­”æ¡ˆã€‚")
                        st.info(f"æœ¬æ¬¡è¿è¡Œæ¶ˆè€—çš„Tokens: {all_total_tokens}")
        except Exception as e:
            st.toast(f"è¿è¡Œå‡ºé”™: {str(e)}", icon='ğŸš«')
            st.error(f"è¿è¡Œå‡ºé”™: {str(e)}")
